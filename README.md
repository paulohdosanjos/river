Nesse projeto buscamos estudar formas nas quais um agente inteligente pode explicar seu resultado para o usuário.

# Explicacação

## Problemas de interpretabilidade

 Aqui, consideramos primeiramente eventos E(A) da forma: E(A) = {h : existe t tal que h(t) = A}. Ou seja, o conjunto de histórias em que um estado A = (x,y) foi atingido.
 
 Pergunta 1: acontece E? Resp: Probabilidade de acontecer E


 Pergunta 2: se E1, acontece E2? Resp: Probabilidade de acontecer E2 condicionado a E1


## Problemas de argumentação

Pergunta 3: porque não x?

Se x, custo acumulado vai para tanto.
Se função recompensa fosse f', então seria x.
Se função de transição fosse T', então seria x.


Pergunta 4: porque y?

Se não y, custo acumulado vai para tanto.
Se função recompensa fosse f', então não seria y.
Se função de transição fosse T', então não seria y.


